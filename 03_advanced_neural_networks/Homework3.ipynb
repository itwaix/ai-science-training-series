{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:57:58.636129Z",
     "start_time": "2024-04-09T04:57:58.623129Z"
    }
   },
   "cell_type": "code",
   "source": "import torch, torchvision",
   "id": "4e503111dd38d88f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:00.808714Z",
     "start_time": "2024-04-09T04:57:58.654635Z"
    }
   },
   "source": [
    "from torchvision.transforms import v2\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToTensor(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=32, scale=[0.85,1.0], antialias=False),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               pin_memory=True,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data,\n",
    "                                             batch_size=batch_size,\n",
    "                                             pin_memory=True,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:00.823478Z",
     "start_time": "2024-04-09T04:58:00.811691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "cf430ce9fea1fd0c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:21.669545Z",
     "start_time": "2024-04-09T04:58:00.826092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch, (X, Y) = next(enumerate(train_dataloader))\n",
    "plt.imshow(X[0].cpu().permute((1,2,0))); plt.show()"
   ],
   "id": "88099f73c0a0b59d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:21.699737Z",
     "start_time": "2024-04-09T04:58:21.675687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # CIFAR-10 is *color* images so 3 layers!\n",
    "    return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "\n",
    "train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n",
    "val_dataloader = WrappedDataLoader(val_dataloader, preprocess)"
   ],
   "id": "3e11892a8a56fe06",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change the network architecture. I will adjust the number of convolutional layers, the number of filters in each layer, and also tweak the initial \"patchify\" layer.",
   "id": "1cd670421d1d5580"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:21.731785Z",
     "start_time": "2024-04-09T04:58:21.703739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shape, stride=2):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size = stride,\n",
    "            stride = stride,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "\n",
    "        return self.downsample(self.norm(inputs))\n",
    "\n",
    "\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, shape):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=in_channels,\n",
    "                               groups=in_channels,\n",
    "                               kernel_size=[7,7],\n",
    "                               padding='same' )\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=4*in_channels,\n",
    "                               kernel_size=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4*in_channels,\n",
    "                               out_channels=in_channels,\n",
    "                               kernel_size=1\n",
    "                               )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage, dropout_rate=0.5):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                              out_channels=n_initial_filters,\n",
    "                              kernel_size=3,  # Changed kernel size\n",
    "                              stride=1,\n",
    "                              padding=1)  # Added padding to maintain size\n",
    "\n",
    "        current_shape = [32, 32]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([n_initial_filters, *current_shape])\n",
    "\n",
    "        current_n_filters = n_initial_filters\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for i, n_blocks in enumerate(range(n_stages)):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n",
    "            # Add a downsampling layer:\n",
    "            if i != n_stages - 1:\n",
    "                # Skip downsampling if it's the last layer!\n",
    "                self.layers.append(Downsampler(\n",
    "                    in_channels=current_n_filters,\n",
    "                    out_channels=2*current_n_filters,\n",
    "                    shape=current_shape,\n",
    "                )\n",
    "                )\n",
    "                # Double the number of filters:\n",
    "                current_n_filters = 2*current_n_filters\n",
    "                # Cut the shape in half:\n",
    "                current_shape = [cs // 2 for cs in current_shape]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # AdaptiveAvgPool2d instead of avg_pool2d\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),  # Added dropout layer\n",
    "            nn.Linear(current_n_filters, 256),  # Changed output size\n",
    "            nn.ReLU(inplace=True),  # ReLU activation function\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.stem(inputs)\n",
    "        x = self.norm1(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "        # x = self.norm2(x)\n",
    "        # x = self.bottleneck(x)\n",
    "\n",
    "        # # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:\n",
    "        # return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))"
   ],
   "id": "b69c0b8f3e5d18bf",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:22.457161Z",
     "start_time": "2024-04-09T04:58:21.733786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Classifier(64, 4, 2)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n"
   ],
   "id": "4b7c26d1debc7ca7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Classifier                               [128, 10]                 --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─LayerNorm: 1-2                         [128, 64, 32, 32]         131,072\n",
      "├─Sequential: 1-3                        [128, 512, 4, 4]          --\n",
      "│    └─ConvNextBlock: 2-1                [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 32, 32]         3,200\n",
      "│    │    └─LayerNorm: 3-2               [128, 64, 32, 32]         131,072\n",
      "│    │    └─Conv2d: 3-3                  [128, 256, 32, 32]        16,640\n",
      "│    │    └─Conv2d: 3-4                  [128, 64, 32, 32]         16,448\n",
      "│    └─ConvNextBlock: 2-2                [128, 64, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-5                  [128, 64, 32, 32]         3,200\n",
      "│    │    └─LayerNorm: 3-6               [128, 64, 32, 32]         131,072\n",
      "│    │    └─Conv2d: 3-7                  [128, 256, 32, 32]        16,640\n",
      "│    │    └─Conv2d: 3-8                  [128, 64, 32, 32]         16,448\n",
      "│    └─Downsampler: 2-3                  [128, 128, 16, 16]        --\n",
      "│    │    └─LayerNorm: 3-9               [128, 64, 32, 32]         131,072\n",
      "│    │    └─Conv2d: 3-10                 [128, 128, 16, 16]        32,896\n",
      "│    └─ConvNextBlock: 2-4                [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-11                 [128, 128, 16, 16]        6,400\n",
      "│    │    └─LayerNorm: 3-12              [128, 128, 16, 16]        65,536\n",
      "│    │    └─Conv2d: 3-13                 [128, 512, 16, 16]        66,048\n",
      "│    │    └─Conv2d: 3-14                 [128, 128, 16, 16]        65,664\n",
      "│    └─ConvNextBlock: 2-5                [128, 128, 16, 16]        --\n",
      "│    │    └─Conv2d: 3-15                 [128, 128, 16, 16]        6,400\n",
      "│    │    └─LayerNorm: 3-16              [128, 128, 16, 16]        65,536\n",
      "│    │    └─Conv2d: 3-17                 [128, 512, 16, 16]        66,048\n",
      "│    │    └─Conv2d: 3-18                 [128, 128, 16, 16]        65,664\n",
      "│    └─Downsampler: 2-6                  [128, 256, 8, 8]          --\n",
      "│    │    └─LayerNorm: 3-19              [128, 128, 16, 16]        65,536\n",
      "│    │    └─Conv2d: 3-20                 [128, 256, 8, 8]          131,328\n",
      "│    └─ConvNextBlock: 2-7                [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-21                 [128, 256, 8, 8]          12,800\n",
      "│    │    └─LayerNorm: 3-22              [128, 256, 8, 8]          32,768\n",
      "│    │    └─Conv2d: 3-23                 [128, 1024, 8, 8]         263,168\n",
      "│    │    └─Conv2d: 3-24                 [128, 256, 8, 8]          262,400\n",
      "│    └─ConvNextBlock: 2-8                [128, 256, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-25                 [128, 256, 8, 8]          12,800\n",
      "│    │    └─LayerNorm: 3-26              [128, 256, 8, 8]          32,768\n",
      "│    │    └─Conv2d: 3-27                 [128, 1024, 8, 8]         263,168\n",
      "│    │    └─Conv2d: 3-28                 [128, 256, 8, 8]          262,400\n",
      "│    └─Downsampler: 2-9                  [128, 512, 4, 4]          --\n",
      "│    │    └─LayerNorm: 3-29              [128, 256, 8, 8]          32,768\n",
      "│    │    └─Conv2d: 3-30                 [128, 512, 4, 4]          524,800\n",
      "│    └─ConvNextBlock: 2-10               [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-31                 [128, 512, 4, 4]          25,600\n",
      "│    │    └─LayerNorm: 3-32              [128, 512, 4, 4]          16,384\n",
      "│    │    └─Conv2d: 3-33                 [128, 2048, 4, 4]         1,050,624\n",
      "│    │    └─Conv2d: 3-34                 [128, 512, 4, 4]          1,049,088\n",
      "│    └─ConvNextBlock: 2-11               [128, 512, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-35                 [128, 512, 4, 4]          25,600\n",
      "│    │    └─LayerNorm: 3-36              [128, 512, 4, 4]          16,384\n",
      "│    │    └─Conv2d: 3-37                 [128, 2048, 4, 4]         1,050,624\n",
      "│    │    └─Conv2d: 3-38                 [128, 512, 4, 4]          1,049,088\n",
      "├─Sequential: 1-4                        [128, 10]                 --\n",
      "│    └─AdaptiveAvgPool2d: 2-12           [128, 512, 1, 1]          --\n",
      "│    └─Flatten: 2-13                     [128, 512]                --\n",
      "│    └─Dropout: 2-14                     [128, 512]                --\n",
      "│    └─Linear: 2-15                      [128, 256]                131,328\n",
      "│    └─ReLU: 2-16                        [128, 256]                --\n",
      "│    └─Linear: 2-17                      [128, 10]                 2,570\n",
      "==========================================================================================\n",
      "Total params: 7,352,842\n",
      "Trainable params: 7,352,842\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 39.68\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 2072.26\n",
      "Params size (MB): 29.41\n",
      "Estimated Total Size (MB): 2103.24\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:22.472981Z",
     "start_time": "2024-04-09T04:58:22.459123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(dataloader, model, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            val_bar.update()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= (size*batch_size)\n",
    "\n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ],
   "id": "270ebfecf73520a1",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:22.488991Z",
     "start_time": "2024-04-09T04:58:22.475997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update()"
   ],
   "id": "4661255335374cca",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:22.504507Z",
     "start_time": "2024-04-09T04:58:22.491505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
   ],
   "id": "6148deffb5cf4e5d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T04:58:32.931763Z",
     "start_time": "2024-04-09T04:58:22.507510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "for j in range(epochs):\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
    "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
    "\n",
    "    # checking on the training loss and accuracy once per epoch\n",
    "\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
    "        acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
    "\n",
    "        print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
    "    with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "\n",
    "        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n",
    "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")\n",
    "    "
   ],
   "id": "4928aac247a6cd8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train Epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62dd8279d4814fceb4919d26e435b73f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_dataloader), position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m train_bar:\n\u001B[1;32m----> 7\u001B[0m         \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_bar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# checking on the training loss and accuracy once per epoch\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_dataloader), position\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidate (train) Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m train_eval:\n",
      "Cell \u001B[1;32mIn[23], line 3\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(dataloader, model, loss_fn, optimizer, progress_bar)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_one_epoch\u001B[39m(dataloader, model, loss_fn, optimizer, progress_bar):\n\u001B[0;32m      2\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[0;32m      5\u001B[0m         pred \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[0;32m      6\u001B[0m         loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, y)\n",
      "Cell \u001B[1;32mIn[19], line 19\u001B[0m, in \u001B[0;36mWrappedDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdl:\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc(\u001B[38;5;241m*\u001B[39mb))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    385\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[1;32m--> 387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_MultiProcessingDataLoaderIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m   1033\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[1;32m-> 1040\u001B[0m \u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\multiprocessing\\process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[0;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    120\u001B[0m _cleanup()\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\multiprocessing\\context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Popen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\multiprocessing\\context.py:336\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_win32\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     92\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(prep_data, to_child)\n\u001B[1;32m---> 93\u001B[0m     \u001B[43mreduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_child\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     95\u001B[0m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\multiprocessing\\reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdump\u001B[39m(obj, file, protocol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     \u001B[43mForkingPickler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
