{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Print the top 100 most frequent word tokens\n",
    "\n",
    "This part does not use AI; it's merely a lexical analysis matching. The regular expression part references both ChatGPT and Wiki content.\n",
    "\n",
    "Regular expression: https://en.wikipedia.org/wiki/Regular_expression"
   ],
   "id": "78ea3e923bc93e3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T05:16:32.388230Z",
     "start_time": "2024-04-09T05:16:31.716684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "wdict = defaultdict(int)\n",
    "\n",
    "with open('Life_On_The_Mississippi.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    for line in file:\n",
    "        tokens = preprocess_and_tokenize(line)\n",
    "        for token in tokens:\n",
    "            wdict[token] += 1\n",
    "\n",
    "sorted_wdict = sorted(wdict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 100 most frequent word tokens:\")\n",
    "for i, (word, freq) in enumerate(sorted_wdict[:100], 1):\n",
    "    print(f\"{i}. {word}: {freq}\")\n",
    "\n",
    "# Calculate number of distinct words making up the top 90% of word occurrences\n",
    "total_occurrences = sum(freq for _, freq in sorted_wdict)\n",
    "threshold_occurrences = total_occurrences * 0.9\n",
    "cumulative_occurrences = 0\n",
    "distinct_words_count = 0\n",
    "for word, freq in sorted_wdict:\n",
    "    cumulative_occurrences += freq\n",
    "    distinct_words_count += 1\n",
    "    if cumulative_occurrences >= threshold_occurrences:\n",
    "        break\n",
    "\n",
    "print(f\"\\nNumber of distinct words making up the top 90% of word occurrences: {distinct_words_count}\")\n"
   ],
   "id": "f3aa9daff2a8805b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 most frequent word tokens:\n",
      "1. 2: 17\n",
      "2. 1: 14\n",
      "3. 4: 9\n",
      "4. ebook: 4\n",
      "5. ##6: 4\n",
      "6. ##1: 4\n",
      "7. 3: 4\n",
      "8. ##enberg: 3\n",
      "9. 85: 3\n",
      "10. 8: 3\n",
      "11. ##3: 3\n",
      "12. project: 2\n",
      "13. gut: 2\n",
      "14. 22: 2\n",
      "15. of: 2\n",
      "16. ##9: 2\n",
      "17. 5: 2\n",
      "18. the: 2\n",
      "19. this: 2\n",
      "20. 107: 2\n",
      "21. ##7: 2\n",
      "22. 36: 2\n",
      "23. states: 2\n",
      "24. 56: 2\n",
      "25. ##2: 2\n",
      "26. most: 2\n",
      "27. 67: 2\n",
      "28. you: 2\n",
      "29. it: 2\n",
      "30. 10: 2\n",
      "31. 7: 2\n",
      "32. produced: 2\n",
      "33. u: 1\n",
      "34. ##fe: 1\n",
      "35. ##ff: 1\n",
      "36. ##the: 1\n",
      "37. 79: 1\n",
      "38. 44: 1\n",
      "39. life: 1\n",
      "40. on: 1\n",
      "41. 84: 1\n",
      "42. ##43: 1\n",
      "43. mississippi: 1\n",
      "44. 104: 1\n",
      "45. 127: 1\n",
      "46. is: 1\n",
      "47. for: 1\n",
      "48. 101: 1\n",
      "49. use: 1\n",
      "50. 34: 1\n",
      "51. anyone: 1\n",
      "52. anywhere: 1\n",
      "53. in: 1\n",
      "54. 238: 1\n",
      "55. united: 1\n",
      "56. 26: 1\n",
      "57. and: 1\n",
      "58. 119: 1\n",
      "59. other: 1\n",
      "60. 223: 1\n",
      "61. parts: 1\n",
      "62. world: 1\n",
      "63. 40: 1\n",
      "64. at: 1\n",
      "65. no: 1\n",
      "66. 325: 1\n",
      "67. cost: 1\n",
      "68. 18: 1\n",
      "69. with: 1\n",
      "70. 105: 1\n",
      "71. almost: 1\n",
      "72. 37: 1\n",
      "73. restrictions: 1\n",
      "74. whatsoever: 1\n",
      "75. 92: 1\n",
      "76. may: 1\n",
      "77. copy: 1\n",
      "78. 12: 1\n",
      "79. 199: 1\n",
      "80. give: 1\n",
      "81. 138: 1\n",
      "82. away: 1\n",
      "83. or: 1\n",
      "84. re: 1\n",
      "85. ##use: 1\n",
      "86. under: 1\n",
      "87. 112: 1\n",
      "88. terms: 1\n",
      "89. license: 1\n",
      "90. included: 1\n",
      "91. 59: 1\n",
      "92. online: 1\n",
      "93. www: 1\n",
      "94. ##gut: 1\n",
      "95. ##org: 1\n",
      "96. if: 1\n",
      "97. 81: 1\n",
      "98. are: 1\n",
      "99. not: 1\n",
      "100. 680: 1\n",
      "\n",
      "Number of distinct words making up the top 90% of word occurrences: 129\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef6c02a21d006d5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I've tried running the code you provided, but I'm unable to display the plot. When I use BERT, I encounter the following error:\n",
    "\n",
    "\"RuntimeError: The expanded size of the tensor (753) must match the existing size (512) at non-singleton dimension 1. Target sizes: [1, 753]. Tensor sizes: [1, 512]\""
   ],
   "id": "69d484ae70d30b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T05:17:21.060200Z",
     "start_time": "2024-04-09T05:16:32.390228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "\n",
    "with open('Life_On_The_Mississippi.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "word_freq = {}\n",
    "for token in tokens:\n",
    "    if token in word_freq:\n",
    "        word_freq[token] += 1\n",
    "    else:\n",
    "        word_freq[token] = 1\n",
    "\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "selected_tokens = [token for token, _ in sorted_word_freq[:10000]]  # Select top 10,000 tokens\n",
    "encoded_inputs = tokenizer(selected_tokens, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "tsne = TSNE(n_components=2, early_exaggeration=12, verbose=2, metric='cosine', init='pca', n_iter=2500)\n",
    "tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(len(tsne_embeddings)):\n",
    "    plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1], s=10)\n",
    "    plt.annotate(selected_tokens[i], (tsne_embeddings[i, 0], tsne_embeddings[i, 1]), alpha=0.5)\n",
    "plt.show()\n"
   ],
   "id": "e224e41ea8864031",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eaaef78dcb40465c9aaa5ce527c827f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itwai\\anaconda3\\envs\\ai\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\itwai\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82438043258e499d86f6fd64d6cb9ceb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3bf15ea448c4736b8b8a676ef6cc191"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b20a1640488540e28bc639504eac2560"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3462df9f57e4d0c8d74ef3a1ed0fcee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. TSNE expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 39\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# Perform t-SNE projection\u001B[39;00m\n\u001B[0;32m     38\u001B[0m tsne \u001B[38;5;241m=\u001B[39m TSNE(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, early_exaggeration\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, metric\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcosine\u001B[39m\u001B[38;5;124m'\u001B[39m, init\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpca\u001B[39m\u001B[38;5;124m'\u001B[39m, n_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2500\u001B[39m)\n\u001B[1;32m---> 39\u001B[0m tsne_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mtsne\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Visualize t-SNE projection for selected words\u001B[39;00m\n\u001B[0;32m     42\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m20\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 295\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    296\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    297\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    298\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    299\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    300\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    301\u001B[0m         )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1136\u001B[0m, in \u001B[0;36mTSNE.fit_transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m   1115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001B[39;00m\n\u001B[0;32m   1116\u001B[0m \n\u001B[0;32m   1117\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1133\u001B[0m \u001B[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params_vs_input(X)\n\u001B[1;32m-> 1136\u001B[0m embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_ \u001B[38;5;241m=\u001B[39m embedding\n\u001B[0;32m   1138\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:866\u001B[0m, in \u001B[0;36mTSNE._fit\u001B[1;34m(self, X, skip_num_points)\u001B[0m\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate\n\u001B[0;32m    865\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbarnes_hut\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 866\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    870\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    871\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    872\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    873\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[0;32m    874\u001B[0m         X, accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoo\u001B[39m\u001B[38;5;124m\"\u001B[39m], dtype\u001B[38;5;241m=\u001B[39m[np\u001B[38;5;241m.\u001B[39mfloat32, np\u001B[38;5;241m.\u001B[39mfloat64]\n\u001B[0;32m    875\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\base.py:633\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    631\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 633\u001B[0m     out \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[0;32m    635\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\utils\\validation.py:1043\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m   1038\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1039\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1040\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1041\u001B[0m     )\n\u001B[0;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nd \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[1;32m-> 1043\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1044\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1045\u001B[0m         \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m   1046\u001B[0m     )\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m   1049\u001B[0m     _assert_all_finite(\n\u001B[0;32m   1050\u001B[0m         array,\n\u001B[0;32m   1051\u001B[0m         input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[0;32m   1052\u001B[0m         estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[0;32m   1053\u001B[0m         allow_nan\u001B[38;5;241m=\u001B[39mforce_all_finite \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1054\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found array with dim 3. TSNE expected <= 2."
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
